# **Sequence Classification with Hugging Face Transformers**

A full end-to-end project for building, training, evaluating, and deploying a state-of-the-art text classification model using the ü§ó Transformers and ü§ó Datasets libraries.

This project expands the official Hugging Face tutorial:
üîó [Sequence Classication Guide](https://huggingface.co/docs/transformers/tasks/sequence_classification)
and organizes it into a complete, production-ready workflow.

---

# **üìå Project Overview**

The goal of this project is to fine-tune a pretrained transformer model (e.g., BERT, RoBERTa, DistilBERT) on a text classification task such as sentiment analysis, topic classification, or spam detection.

This repository includes:

* Dataset loading and preprocessing
* Tokenization and input preparation
* Model fine-tuning with Trainer
* Metrics + error analysis
* Batch inference and deployment scripts
* Model saving, versioning, and Hub integration
* Experiment tracking
* Optional: ONNX export and optimization

---

# **üìÅ Repository Structure**

```
project-root/
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_dataset_exploration.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_tokenization.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_finetune_model.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 04_evaluation_and_error_analysis.ipynb
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ train.py             # training entrypoint
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py          # evaluation entrypoint
‚îÇ   ‚îú‚îÄ‚îÄ predict.py           # batch + single-text predictions
‚îÇ   ‚îî‚îÄ‚îÄ export_onnx.py       # optional ONNX conversion
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                 # untouched dataset
‚îÇ   ‚îú‚îÄ‚îÄ processed/           # tokenized/imputed data
‚îÇ   ‚îî‚îÄ‚îÄ splits/              # reproducible train/val/test
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/         # intermediate checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ final/               # best model for production
‚îÇ   ‚îî‚îÄ‚îÄ hub/                 # ready-to-push Hugging Face repo
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ metrics.json
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png
‚îÇ   ‚îú‚îÄ‚îÄ classification_report.txt
‚îÇ   ‚îî‚îÄ‚îÄ predictions.csv
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py           # accuracy, f1, etc.
‚îÇ   ‚îú‚îÄ‚îÄ plotting.py          # confusion matrix, ROC, etc.
‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py     # dataset transforms
‚îÇ
‚îú‚îÄ‚îÄ environment.yml or requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

# **‚öôÔ∏è Setup**

### **1. Create & activate environment**

> [Install Miniconda if not already installed](https://www.anaconda.com/docs/getting-started/miniconda/install#macos-2)

```bash
conda env create -f environment.yml # If not already created
conda activate hf-seq-classification
```

__Authenticate with your HuggingFace token to push to the Hub__:
```bash
huggingface-cli login
```

---

# **üß† Training Workflow**

## **1. Load dataset**

Example: IMDB sentiment classification

```python
from datasets import load_dataset
dataset = load_dataset("imdb")
```

## **2. Tokenize**

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def preprocess(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=256
    )
```

## **3. Fine-tune the model**

```bash
python scripts/train.py \
  --model_name bert-base-uncased \
  --dataset_name imdb \
  --output_dir models/checkpoints \
  --epochs 3 \
  --batch_size 16 \
  --learning_rate 3e-5
```

Training includes:

* mixed precision (AMP)
* evaluation at end of each epoch
* optional logging via TensorBoard or Weights & Biases

---

# **üìä Evaluation & Analysis**

Run:

```bash
python scripts/evaluate.py \
  --model_dir models/final \
  --dataset_name imdb
```

Produces:

* Accuracy, Precision, Recall, F1
* Confusion matrix (`results/confusion_matrix.png`)
* Classification report
* Misclassified example dump

Error analysis details which examples the model struggles with (sarcasm, negation, domain shifts, etc.).

---

# **üîç Inference**

### **Single text**

```bash
python scripts/predict.py --text "I loved this movie!"
```

### **Batch inference**

```bash
python scripts/predict.py --file path/to/file.csv
```

Outputs a CSV with predicted labels + probabilities.

---

# **‚òÅÔ∏è Pushing to Hugging Face Hub**

1. Login:

```bash
huggingface-cli login
```

2. Push:

```python
trainer.push_to_hub()
```

or manually:

```python
model.push_to_hub("your-username/your-model-name")
tokenizer.push_to_hub("your-username/your-model-name")
```

The final model lives under `models/hub/`.

---

# **üöÄ Deployment Options**

### Option A ‚Äî Use Hugging Face Inference API

Instant hosting via:

```
https://huggingface.co/<username>/<model>/api
```

### Option B ‚Äî Local FastAPI inference server

### Option C ‚Äî Export to ONNX and serve with ONNX Runtime

```bash
python scripts/export_onnx.py
```

---

# **üß™ Experiment Tracking**

You can optionally integrate:

* **TensorBoard**
* **Weights & Biases**
* **MLflow**

Example Trainer config:

```python
TrainingArguments(
    ...,
    report_to="tensorboard",
)
```

---

# **üîÅ Reproducibility**

This repo uses:

* fixed seeds
* stored environment files (requirements/conda env)
* deterministic train/val/test splits
* versioned checkpoints
* `config.json` for model metadata

---

# **üôå Contributing**

Contributions welcome. Ideas include:

* adding new datasets
* experimenting with different architectures
* adding ONNX Runtime benchmarks
* exploring quantization
* improving evaluation visualizations

---

# **üìé References**

* Hugging Face Transformers: [https://huggingface.co/docs/transformers](https://huggingface.co/docs/transformers)
* Hugging Face Datasets: [https://huggingface.co/docs/datasets](https://huggingface.co/docs/datasets)
* Evaluation: [https://huggingface.co/docs/evaluate](https://huggingface.co/docs/evaluate)
* Accelerate: [https://huggingface.co/docs/accelerate](https://huggingface.co/docs/accelerate)

---
